```{r "setup", include=FALSE}
require("knitr")
opts_knit$set(root.dir = "/Users/vsriniv6/Documents/paris_data")
```

```{r}
options(stringsAsFactors = FALSE)
Sys.setlocale(category = "LC_ALL", locale = "en_US.UTF-8")
```

```{r}
#rm(data,data_non_solidarity,data_solidarity,emDict_raw)
#rm(list = ls()) 
#install.packages("slam")
library(slam)
library(textcat)
#library(cldr)
library(entropart)
library(boot)
library(vegan)
library(simboot)
#update.packages()
library(tidyverse)
#library(tokenizers)
library(mgcv)
library(twitteR)
library(plyr)
library(dplyr)
library(ROAuth)
library(stringr)
library(ggplot2)
library(httr)
library(wordcloud)
library(stringi)
#library(sentiment)
library(SnowballC)
library(tm)
library(RColorBrewer)
```


```{r}
#setwd("/Users/vsriniv6/Documents/paris_data")
data = read.csv2(file="paris_final_0822.csv", header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
```



```{r}
names(data)
```



```{r}
nrow(data)
```

```{r}
unique(data$user_lang)
```



```{r}
lang_groups= data%>%
  group_by(user_lang=data$user_lang) %>%
  dplyr::summarize(n=n())
print(sum(lang_groups$n))
```



```{r}
library(ggplot2)
library(scales)
ggplot(lang_groups, aes(x = reorder(user_lang, -n), y = n)) +
         geom_bar(stat="identity") +
        theme(axis.text.x=element_text(angle = -90, hjust = 0)) +
        scale_y_continuous(labels = comma)
```

We can see that English, Spanish and French are the top three languages in paris dataset. Let's analyse the entropy of these three languages

```{r}
data_new= data%>%
  group_by(user_lang=data$user_lang) 
```
  
  
  
  
```{r}  
corpus_en_emoji <- corpus_en[which(str_detect(corpus_en,"[^[:ascii:]]|(?::|;|=)(?:-)?(?:\\)|\\(|D|P)")==T)]
length(corpus_en_emoji)
emoji_corpus <- corpus_en_emoji
##tweet without emoji/emoticons
corpus_en_NO_emoji <- subset(corpus_en, !(corpus_en %in% c(corpus_en_emoji)))
dplyr::filter(!str_detect(orig_tweets,"(RT|via)((?:\\b\\W*@\\w+)+)"))
length(corpus_en_NO_emoji)
length(corpus_en)
```

  
  
```{r}
#rm(data,lang_groups)
#rm(data)
#data_en <- subset(data_new,user_lang == "en")
corpus_en <-  data_en$tweet
##tweet with emoji/emoticons
length(corpus_en)
corpus_en <- unique(corpus_en)
length(corpus_en)
corpus_en <- text$orig_tweets
length(corpus_en)
```

```{r}
corpus_en_emoji <- group_raw_tweets$text
corpus_en_NO_emoji_text <- subset(corpus_en, !(corpus_en %in% c(corpus_en_emoji)))
corpus_en_NO_emoji_text <- as.data.frame(unique(corpus_en_NO_emoji_text))
names(corpus_en_NO_emoji_text)[1]<-"tweets"
corpus_en_NO_emoji_text <- corpus_en_NO_emoji_text %>%dplyr::filter(!str_detect(tweets,"(RT|via)((?:\\b\\W*@\\w+)+)"))
corpus_en_NO_emoji <- corpus_en_NO_emoji_text$tweets
length(corpus_en_emoji)
length(corpus_en_NO_emoji)
```



```{r}
write.csv(corpus_en_emoji,"paris_en_emoji.csv")
write.csv(corpus_en_NO_emoji,"paris_en_no_emoji.csv")
```

##English tweets 

```{r}
 ##Remove emoji
corpus_en_full <- iconv(corpus_en, "latin1", "ASCII", sub="")

### remove retweet entities
corpus_en_full <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_en_full) 


corpus_en_full <- Corpus(VectorSource(corpus_en_full))



##convert text to lowercase
corpus_en_full<- tm_map(corpus_en_full,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_en_full <- tm_map(corpus_en_full,removePunctuation)


##remove numbers from text
corpus_en_full <- tm_map(corpus_en_full,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_en <- tm_map(corpus_en_full,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])

##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_en <- tm_map(cleaned_corpus_en,content_transformer(removeEmoticons))
#len(cleaned_corpus_en)

   
##remove white space
cleaned_corpus_en <- tm_map(cleaned_corpus_en,stripWhitespace)
###


cleaned_corpus_en <- tm_map(cleaned_corpus_en,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_en$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_en <- tm_map(cleaned_corpus_en,stemDocument)
cleaned_corpus_en <- tm_map(cleaned_corpus_en,stripWhitespace)
#len(cleaned_corpus_en)
##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_en,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
length(cleaned_corpus_en)
#inspect(cleaned_corpus_en_emoji[1:25])

```


##Average word length is 4.8 and average tweet length is 12.5 

##Entropy for english tweets 

```{r}
rm(words_list,wsize_per_tweet)
tdm <- TermDocumentMatrix(cleaned_corpus_en)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_en <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_en)/log(2)
H
```
confidence intervals


```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_en, datatype = "abundance", conf = 0.95)/log(2)
```


```{r}
var
```

Error bars with 95% confidence Intervals
```{r}
wf <- data.frame(word=names(freqSum_en), freq=freqSum_en)   
head(wf)
```
```{r}
wf_new <-wf[which(wf$freq>1000),]
```


```{r}
#install.packages("Hmisc")
ggplot(data=wf_new, aes(x=word, y=freq, fill=word))+ stat_summary(fun.y = mean, geom = "bar") + stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.3) + labs(x = "words", y = "frequency")

```


The entropy of english tweets is 10.47

```{r}
rm(tdm,cleaned_corpus_en)
2^H
```

The perplexity of english tweets is 1420.


##English tweets with emoji/emoticons

```{r}
 ##Remove emoji
corpus_en_emoji <- iconv(corpus_en_emoji, "latin1", "ASCII", sub="")

### remove retweet entities
corpus_en_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_en_emoji) 


corpus_en_emoji <- Corpus(VectorSource(corpus_en_emoji))



##convert text to lowercase
corpus_en_emoji <- tm_map(corpus_en_emoji,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_en_emoji <- tm_map(corpus_en_emoji,removePunctuation)


##remove numbers from text
corpus_en_emoji <- tm_map(corpus_en_emoji,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_en_emoji <- tm_map(corpus_en_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])

##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,content_transformer(removeEmoticons))


   
##remove white space
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,stripWhitespace)
###

cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,removeWords,c('paris'))
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_en_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,stemDocument)
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_en_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
length(cleaned_corpus_en_emoji)
inspect(cleaned_corpus_en_emoji[1:25])

```



##Average word length is 4.8 and average tweet length is 14.4 with emoji/emoticons

##Entropy for tweets with emoji/emoticons

```{r}
rm(words_list,wsize_per_tweet,wf,wf_new)
tdm <- TermDocumentMatrix(cleaned_corpus_en_emoji)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_en <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_en)/log(2)
H
```

confidence intervals


```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_en, datatype = "abundance", conf = 0.95)/log(2)
```


```{r}
var
```


Entropy of english  tweets with emoji/emoticons is 10.59


```{r}
rm(tdm,cleaned_corpus_en_emoji)
2^H
```
##Perplexity of english tweets with emoji/emoticons  is 1544

##Emoji's per tweet

```{r}
count_matches <- function(string, matchto, description, sentiment = NA) {
  
  vec <- str_count(string, matchto)
  matches <- which(vec != 0)
  descr <- ifelse((length(matches) != 0),description[matches],NA)
  cnt <- ifelse((length(matches) != 0),vec[matches],NA)
  df <- data.frame(text = string, description = descr, count = cnt)
  
  df$sentiment <- ifelse((!is.na(sentiment) & length(sentiment[matches]) != 0),sentiment[matches],NA)
    

  
  return(df)
  
}
```


```{r}
emojis_matching <- function(texts, matchto, description, sentiment = NA) {
  texts %>% 
    lapply(count_matches, matchto = matchto, description = description, sentiment = sentiment) %>%
    bind_rows
}
```


```{r}
parallel_match<- function(texts, matchto, description, sentiment = NA, mc.cores = 4) {
emojis_matching <- function(txt,matchto, description, sentiment = NA) {
  txt %>%  lapply(count_matches, matchto = matchto, description = description, sentiment = sentiment) %>%
    bind_rows
}
mclapply(X = texts, FUN = emojis_matching, matchto, description, sentiment = NA, mc.cores = mc.cores) %>%
    bind_rows

}
```

```{r}
rm(emDict_raw,matchto,description,new_data_en,ex)
```


```{r}
emDict_raw <- read.csv2("emDict.csv") %>% 
      select(EN, utf8, unicode) %>% 
      dplyr::rename(description = EN, r_encoding = utf8)
```

```{r}
# plain skin tones
skin_tones <- c("light skin tone", 
                "medium-light skin tone", 
                "medium skin tone",
                "medium-dark skin tone", 
                "dark skin tone")

# remove plain skin tones and remove skin tone info in description
emDict <- emDict_raw %>%
  # remove plain skin tones emojis
  filter(!description %in% skin_tones) %>%
  # remove emojis with skin tones info, e.g. remove woman: light skin tone and only
  # keep woman
  filter(!grepl(":", description)) %>%
  mutate(description = tolower(description)) %>%
  mutate(unicode = unicode)
```


```{r}
matchto <- emDict$r_encoding
description <-emDict$description
```


```{r}
#rm(new_data)

new_data_en <- data_en %>% mutate(text=iconv(tweet, from = "latin1", to = "ascii", sub = "byte"))
```


```{r}
is.factor(emDict$r_encoding)
```

```{r}
head(new_data_en)
```

#Average number of emoji's/tweet
```{r}
text <- as.data.frame(unique(new_data_en$text))
names(text)[1]<-"orig_tweets"
```

```{r}
text 
```

```{r}
text <- text %>% dplyr::filter(!str_detect(orig_tweets,"(RT|via)((?:\\b\\W*@\\w+)+)"))
```


```{r}
library(dplyr)
require(parallel) 
raw_tweets <- parallel_match(text$orig_tweets, matchto, description,mc.cores=4)%>% 
  filter(text != "") %>% filter(!is.na(count)) 
  
```


```{r}
head(raw_tweets,20)
```

```{r}
drops <- c("sentiment")
raw_tweets_altered <-raw_tweets[ , !(names(raw_tweets) %in% drops)]
```

```{r}
raw_tweets_altered <- cbind(raw_tweets_altered,unique_count=1)
raw_tweets_altered
```

```{r}
group_raw_tweets <- group_by(raw_tweets_altered,text)%>%dplyr::summarise(count= sum(count),unique_count=sum(unique_count))
group_raw_tweets
```

#Average number of emoji's per tweet - English

```{r}
mean(group_raw_tweets$count, na.rm = TRUE)
```

The average number of emoji's per tweet in English is 1.98

#Average number of unique emoji's per tweet - English

```{r}
mean(group_raw_tweets$unique_count, na.rm = TRUE)
```

The average number of unique emoji's per tweet in English is 1.57

#Emoji entropy - English
```{r}
H=Shannon(raw_tweets$count)/log(2)
H
```
The entropy of emoji's in english is 17.10

#top 50 tweets with emoji's
```{r}
top50_eng_paris <- head(group_raw_tweets[with(group_raw_tweets,order(-count)),],50)
top50_eng_paris
```


```{r}
tweets <-  top50_eng_paris$text
m <- gregexpr("<[0-9a-f]{2}>", tweets)
codes <- regmatches(tweets, m)
chars <- lapply(codes, function(x) {
    rawToChar(as.raw(strtoi(paste0("0x", substr(x,2,3)))), multiple = TRUE)
})

regmatches(tweets, m) <- chars
Encoding(tweets) <- "UTF-8"
tweets
```

```{r}
write.csv(tweets,"top_50_english_paris.csv")
```

##English tweets  without emoji/emoticons
```{r}

### remove retweet entities
 corpus_en_NO_emoji <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_en_NO_emoji) 
 
corpus_en_NO_emoji <- Corpus(VectorSource(corpus_en_NO_emoji))
##convert text to lowercase

corpus_en_NO_emoji <- tm_map(corpus_en_NO_emoji,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_en_NO_emoji <- tm_map(corpus_en_NO_emoji,removePunctuation)


##remove numbers from text
corpus_en_NO_emoji <- tm_map(corpus_en_NO_emoji,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##cleaned_corpus_USA_en_NO_emoji <- tm_map(corpus_USA_en_NO_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_en_NO_emoji <- tm_map(corpus_en_NO_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])

cleaned_corpus_en_NO_emoji <- tm_map(cleaned_corpus_en_NO_emoji,removeWords,c('paris'))
cleaned_corpus_en_NO_emoji <- tm_map(cleaned_corpus_en_NO_emoji,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_en_NO_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_en_NO_emoji <- tm_map(cleaned_corpus_en_NO_emoji,stemDocument)
cleaned_corpus_en_NO_emoji <- tm_map(cleaned_corpus_en_NO_emoji,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_en_NO_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
```


#Average word length is 4.8 and average tweet length is 10.79 without emoji/emoticons


##Entropy analysis for english tweets without emoji/emoticons 

```{r}
rm(corpus_en_NO_emoji,corpus_en,words_list,wsize_per_tweet)
tdm <- TermDocumentMatrix(cleaned_corpus_en_NO_emoji)
#tdm
#tdm <- as.matrix(tdm)
freqSum_en_no_emoji <- slam::row_sums(tdm, na.rm = T)
#freqSum_France_fr 
H=Shannon(freqSum_en_no_emoji)/log(2)
H
```

##Entropy of english tweets  without emoji/emoticons is 10.01

Thus, we can see that entropy for tweets without emoji/emoticons is slightly less than the entropy of tweets containing emoji/emoticons

confidence intervals


```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_en_no_emoji, datatype = "abundance", conf = 0.95)/log(2)
```


```{r}
var
```



##Perplexity
```{r}
rm(tdm,cleaned_corpus_en_NO_emoji,emoji_corpus)
2^H
```

##Perplexity of english tweets without emoji/emoticons  is 1033

French


```{r}
#rm(data,lang_groups)
#rm(data)
data_fr <- subset(data_new,user_lang == "fr")
corpus_fr <-  data_fr$tweet
##tweet with emoji/emoticons
length(corpus_fr)
corpus_fr <- unique(corpus_fr)
length(corpus_fr)
corpus_fr <- text$orig_tweets
length(corpus_fr)
```

```{r}
corpus_fr_emoji <- group_raw_tweets_fr$text
corpus_fr_NO_emoji_text <- subset(corpus_fr, !(corpus_fr %in% c(corpus_fr_emoji)))
corpus_fr_NO_emoji_text <- as.data.frame(unique(corpus_fr_NO_emoji_text))
names(corpus_fr_NO_emoji_text)[1]<-"tweets"
corpus_fr_NO_emoji_text <- corpus_fr_NO_emoji_text %>%dplyr::filter(!str_detect(tweets,"(RT|via)((?:\\b\\W*@\\w+)+)"))
corpus_fr_NO_emoji <- corpus_fr_NO_emoji_text$tweets
length(corpus_fr_emoji)
length(corpus_fr_NO_emoji)
```

```{r}
write.csv(corpus_fr_emoji,"paris_fr_emoji.csv")
write.csv(corpus_fr_NO_emoji,"paris_fr_no_emoji.csv")
```
##French tweets  

```{r}
 ##Remove emoji
corpus_fr_full <- iconv(corpus_fr, "latin1", "ASCII", sub="")

### remove retweet entities
corpus_fr_full <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_fr_full) 


corpus_fr_full <- Corpus(VectorSource(corpus_fr_full))



##convert text to lowercase
corpus_fr_full <- tm_map(corpus_fr_full,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_fr_full <- tm_map(corpus_fr_full,removePunctuation)


##remove numbers from text
corpus_fr_full <- tm_map(corpus_fr_full,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_fr <- tm_map(corpus_fr_full,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])

##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_fr <- tm_map(cleaned_corpus_fr,content_transformer(removeEmoticons))


   
##remove white space
cleaned_corpus_fr <- tm_map(cleaned_corpus_fr,stripWhitespace)
###

cleaned_corpus_fr <- tm_map(cleaned_corpus_fr,removeWords)
cleaned_corpus_fr <- tm_map(cleaned_corpus_fr,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_fr$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_fr <- tm_map(cleaned_corpus_fr,stemDocument)
cleaned_corpus_fr <- tm_map(cleaned_corpus_fr,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_fr,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
length(cleaned_corpus_fr)
#inspect(cleaned_corpus_en_emoji[1:25])

```



##Average word length is 4.9 and average tweet length is 11.8

##Entropy of French tweets

```{r}
rm(words_list,wsize_per_tweet,corpus_fr)
tdm <- TermDocumentMatrix(cleaned_corpus_fr)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_fr <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_fr)/log(2)
H
```
##Entropy of French tweets with emoji/emoticons  is 10.41

confidence intervals


```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_fr, datatype = "abundance", conf = 0.95)/log(2)
```


```{r}
var
```

Error bars with 95% confidence intervals
```{r}
wf <- data.frame(word=names(freqSum_fr), freq=freqSum_fr)   
head(wf)
```

```{r}
wf_new <-wf[which(wf$freq>30000),]
nrow(wf_new)
```


```{r}
#install.packages("Hmisc")
ggplot(data=wf_new, aes(x=word, y=freq, fill=word))+ stat_summary(fun.y = mean, geom = "bar") + stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.3) + labs(x = "words", y = "frequency")

```


##Perplexity



```{r}
rm(tdm,freqSum_fr)
2^H
```


The perplexity for french tweets is 1363


##French tweets  with emoji/emoticons

```{r}
 ##Remove emoji
corpus_fr_emoji <- iconv(corpus_fr_emoji, "latin1", "ASCII", sub="")

### remove retweet entities
corpus_fr_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_fr_emoji) 


corpus_fr_emoji <- Corpus(VectorSource(corpus_fr_emoji))



##convert text to lowercase
corpus_fr_emoji <- tm_map(corpus_fr_emoji,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_fr_emoji <- tm_map(corpus_fr_emoji,removePunctuation)


##remove numbers from text
corpus_fr_emoji <- tm_map(corpus_fr_emoji,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_fr_emoji <- tm_map(corpus_fr_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])

##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_fr_emoji <- tm_map(cleaned_corpus_fr_emoji,content_transformer(removeEmoticons))


   
##remove white space
cleaned_corpus_fr_emoji <- tm_map(cleaned_corpus_fr_emoji,stripWhitespace)
###

cleaned_corpus_fr_emoji <- tm_map(cleaned_corpus_fr_emoji,removeWords)
cleaned_corpus_fr_emoji <- tm_map(cleaned_corpus_fr_emoji,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_fr_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_fr_emoji <- tm_map(cleaned_corpus_fr_emoji,stemDocument)
cleaned_corpus_fr_emoji <- tm_map(cleaned_corpus_fr_emoji,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_fr_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
length(cleaned_corpus_fr_emoji)
#inspect(cleaned_corpus_en_emoji[1:25])

```



##Average word length is 4.9 and average tweet length is 12.9 with emoji/emoticons

##Entropy for tweets with emoji/emoticons

```{r}
#rm(words_list,wsize_per_tweet,corpus_fr)
tdm <- TermDocumentMatrix(cleaned_corpus_fr_emoji)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_fr <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_fr)/log(2)
H
```


##Entropy of French tweets with emoji/emoticons  is 10.31


confidence intervals


```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_fr, datatype = "abundance", conf = 0.95)/log(2)
```


```{r}
var
```




```{r}
#rm(new_data)

new_data_fr <- data_fr %>% mutate(text=iconv(tweet, from = "latin1", to = "ascii", sub = "byte"))
```



```{r}
text <- as.data.frame(unique(new_data_fr$text))
names(text)[1]<-"orig_tweets"
```

```{r}
text 
```

```{r}
text <- text %>% dplyr::filter(!str_detect(orig_tweets,"(RT|via)((?:\\b\\W*@\\w+)+)"))
```



```{r}
library(dplyr)
require(parallel) 
raw_tweets_fr <- parallel_match(text$orig_tweets, matchto, description,mc.cores = 8)%>% 
  filter(text != "") %>% filter(!is.na(count)) 
  
```


```{r}
head(new_data_fr,10)
```


```{r}
head(raw_tweets_fr,20)
```

```{r}
drops <- c("sentiment")
raw_tweets_altered_fr <-raw_tweets_fr[ , !(names(raw_tweets_fr) %in% drops)]
```

```{r}
raw_tweets_altered_fr <- cbind(raw_tweets_altered_fr,unique_count=1)
raw_tweets_altered_fr
```

```{r}
group_raw_tweets_fr <- group_by(raw_tweets_altered_fr,text)%>%dplyr::summarise(count= sum(count),unique_count=sum(unique_count))
group_raw_tweets_fr
```

#Average number of emoji's per tweet - French

```{r}
mean(group_raw_tweets_fr$count, na.rm = TRUE)
```

The average number of emoji's per tweet in French is 2.02

#Average number of unique emoji's per tweet - French

```{r}
mean(group_raw_tweets_fr$unique_count, na.rm = TRUE)
```

The average number of unique emoji's per tweet in French is 1.56

#Emoji entropy - French
```{r}
H=Shannon(raw_tweets_fr$count)/log(2)
H
```
The entropy of emoji's in French is 15.75

#top 50 tweets with emoji's
```{r}
top50_fr_paris <- head(group_raw_tweets_fr[with(group_raw_tweets_fr,order(-count)),],50)
top50_fr_paris
```

```{r}
tweets <-  top50_fr_paris$text
m <- gregexpr("<[0-9a-f]{2}>", tweets)
codes <- regmatches(tweets, m)
chars <- lapply(codes, function(x) {
    rawToChar(as.raw(strtoi(paste0("0x", substr(x,2,3)))), multiple = TRUE)
})

regmatches(tweets, m) <- chars
Encoding(tweets) <- "UTF-8"
tweets
```

```{r}
write.csv(tweets,"top_50_french_paris.csv")
```

##Perplexity
```{r}
rm(tdm,freqSum_fr,raw_tweets)
2^H
```

##Perplexity of French tweets with emoji/emoticons  is 1274

##French tweets  without emoji/emoticons
```{r}

### remove retweet entities
 corpus_fr_NO_emoji <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_fr_NO_emoji) 
 
corpus_fr_NO_emoji <- Corpus(VectorSource(corpus_fr_NO_emoji))
##convert text to lowercase

corpus_fr_NO_emoji <- tm_map(corpus_fr_NO_emoji,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_fr_NO_emoji <- tm_map(corpus_fr_NO_emoji,removePunctuation)


##remove numbers from text
corpus_fr_NO_emoji <- tm_map(corpus_fr_NO_emoji,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##cleaned_corpus_USA_en_NO_emoji <- tm_map(corpus_USA_en_NO_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_fr_NO_emoji <- tm_map(corpus_fr_NO_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])

cleaned_corpus_fr_NO_emoji <- tm_map(cleaned_corpus_fr_NO_emoji,removeWords)
cleaned_corpus_fr_NO_emoji <- tm_map(cleaned_corpus_fr_NO_emoji,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_fr_NO_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_fr_NO_emoji <- tm_map(cleaned_corpus_fr_NO_emoji,stemDocument)
cleaned_corpus_fr_NO_emoji <- tm_map(cleaned_corpus_fr_NO_emoji,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_fr_NO_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
```


#Average word length is 5.1 and average tweet length is 8.16 without emoji/emoticons


##Entropy analysis for French tweets without emoji/emoticons 

```{r}
tdm <- TermDocumentMatrix(cleaned_corpus_fr_NO_emoji)
#tdm
#tdm <- as.matrix(tdm)
freqSum_fr_no_emoji <- slam::row_sums(tdm, na.rm = T)
#freqSum_France_fr 
H=Shannon(freqSum_fr_no_emoji)/log(2)
H
```

##Entropy of French tweets with emoji/emoticons is 9.99

Thus we can see that entropy for tweets without emoji/emoticons is slightly less than the entropy of tweets containing emoji/emoticons


confidence intervals


```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_fr_no_emoji, datatype = "abundance", conf = 0.95)/log(2)
```


```{r}
var
```

##Perplexity
```{r}
rm(tdm,cleaned_corpus_fr_NO_emoji,freqSum_fr_no_emoji)
2^H
```

##Perplexity of French tweets without emoji/emoticons  is 1023


Spain 

```{r}
#rm(data,lang_groups)
#rm(data)
data_es <- subset(data_new,user_lang == "es")
corpus_es <-  data_es$tweet
length(corpus_es)
##tweet with emoji/emoticons
corpus_es <- unique(corpus_es)
length(corpus_es)
corpus_es <- text$orig_tweets
length(corpus_es)
```

```{r}
corpus_es_emoji <- group_raw_tweets_es$text
corpus_es_NO_emoji_text <- subset(corpus_es, !(corpus_es %in% c(corpus_es_emoji)))
corpus_es_NO_emoji_text <- as.data.frame(unique(corpus_es_NO_emoji_text))
names(corpus_es_NO_emoji_text)[1]<-"tweets"
corpus_es_NO_emoji_text <- corpus_es_NO_emoji_text %>%dplyr::filter(!str_detect(tweets,"(RT|via)((?:\\b\\W*@\\w+)+)"))
corpus_es_NO_emoji <- corpus_es_NO_emoji_text$tweets
length(corpus_es_emoji)
length(corpus_es_NO_emoji)
```

```{r}
write.csv(corpus_es_emoji,"paris_es_emoji.csv")
write.csv(corpus_es_NO_emoji,"paris_es_no_emoji.csv")
```


##Spanish tweets  

```{r}
 ##Remove emoji
corpus_es_full <- iconv(corpus_es, "latin1", "ASCII", sub="")

### remove retweet entities
corpus_es_full <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_es_full) 


corpus_es_full <- Corpus(VectorSource(corpus_es_full))



##convert text to lowercase
corpus_es_full <- tm_map(corpus_es_full,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_es_full <- tm_map(corpus_es_full,removePunctuation)


##remove numbers from text
corpus_es_full <- tm_map(corpus_es_full,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_es <- tm_map(corpus_es_full,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])

##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_es <- tm_map(cleaned_corpus_es,content_transformer(removeEmoticons))


   
##remove white space
cleaned_corpus_es <- tm_map(cleaned_corpus_es,stripWhitespace)
###

cleaned_corpus_es<- tm_map(cleaned_corpus_es,removeWords)
cleaned_corpus_es <- tm_map(cleaned_corpus_es,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_es$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_es <- tm_map(cleaned_corpus_es,stemDocument)
cleaned_corpus_es <- tm_map(cleaned_corpus_es,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_es,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
length(cleaned_corpus_es)
#inspect(cleaned_corpus_en_emoji[1:25])

```



##Average word length is 4.8 and average tweet length is 13.6 with emoji/emoticons

##Entropy of spanish tweets

```{r}
rm(words_list,wsize_per_tweet,corpus_es,data_es)
tdm <- TermDocumentMatrix(cleaned_corpus_es)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_es <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_es)/log(2)
H
```


##Entropy of Spanish tweets is 10.36


confidence intervals


```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_es, datatype = "abundance", conf = 0.95)/log(2)
```


```{r}
var
```

Error bars with 95% confidence intervals
```{r}
wf <- data.frame(word=names(freqSum_es), freq=freqSum_es)   
head(wf)
```


```{r}
wf_new <-wf[which(wf$freq>40000),]
nrow(wf_new)
```


```{r}
#install.packages("Hmisc")
ggplot(data=wf_new, aes(x=word, y=freq, fill=word))+ stat_summary(fun.y = mean, geom = "bar") + stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.3) + labs(x = "words", y = "frequency")

```

##Perplexity



```{r}
rm(tdm,freqSum_es)
2^H
```
The perplexity of spanish tweets is 1319

##Spanish tweets with emoji/emoticons

```{r}
 ##Remove emoji
corpus_es_emoji <- iconv(corpus_es_emoji, "latin1", "ASCII", sub="")

### remove retweet entities
corpus_es_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_es_emoji) 


corpus_es_emoji <- Corpus(VectorSource(corpus_es_emoji))



##convert text to lowercase
corpus_es_emoji <- tm_map(corpus_es_emoji,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_es_emoji <- tm_map(corpus_es_emoji,removePunctuation)


##remove numbers from text
corpus_es_emoji <- tm_map(corpus_es_emoji,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_es_emoji <- tm_map(corpus_es_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])

##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_es_emoji <- tm_map(cleaned_corpus_es_emoji,content_transformer(removeEmoticons))


   
##remove white space
cleaned_corpus_es_emoji <- tm_map(cleaned_corpus_es_emoji,stripWhitespace)
###

cleaned_corpus_es_emoji <- tm_map(cleaned_corpus_es_emoji,removeWords)
cleaned_corpus_es_emoji <- tm_map(cleaned_corpus_es_emoji,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_es_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_es_emoji <- tm_map(cleaned_corpus_es_emoji,stemDocument)
cleaned_corpus_es_emoji <- tm_map(cleaned_corpus_es_emoji,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_es_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
length(cleaned_corpus_es_emoji)
#inspect(cleaned_corpus_en_emoji[1:25])

```



##Average word length is 4.8 and average tweet length is 14.5 with emoji/emoticons

##Entropy for spanish tweets with emoji/emoticons

```{r}
rm(words_list,wsize_per_tweet)
tdm <- TermDocumentMatrix(cleaned_corpus_es_emoji)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_es <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_es)/log(2)
H
```


##Entropy of spanish tweets with emoji/emoticons  is 10.21
##Perplexity


confidence intervals


```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_es, datatype = "abundance", conf = 0.95)/log(2)
```


```{r}
var
```

#Average number of emoji's/tweet


```{r}
#rm(new_data)

new_data_es <- data_es %>% mutate(text=iconv(tweet, from = "latin1", to = "ascii", sub = "byte"))
```


```{r}
text <- as.data.frame(unique(new_data_es$text))
names(text)[1]<-"orig_tweets"
```

```{r}
text 
```

```{r}
text <- text %>% dplyr::filter(!str_detect(orig_tweets,"(RT|via)((?:\\b\\W*@\\w+)+)"))
```



```{r}
library(dplyr)
require(parallel) 
raw_tweets_es<- parallel_match(text$orig_tweets, matchto, description,mc.cores = 8)%>% 
  filter(text != "") %>% filter(!is.na(count)) 
  
```




```{r}
head(raw_tweets_es,20)
```

```{r}
drops <- c("sentiment")
raw_tweets_altered_es <-raw_tweets_es[ , !(names(raw_tweets_es) %in% drops)]
```

```{r}
raw_tweets_altered_es <- cbind(raw_tweets_altered_es,unique_count=1)
raw_tweets_altered_es
```

```{r}
group_raw_tweets_es <- group_by(raw_tweets_altered_es,text)%>%dplyr::summarise(count= sum(count),unique_count=sum(unique_count))
group_raw_tweets_es
```

#Average number of emoji's per tweet - Spanish

```{r}
mean(group_raw_tweets_es$count, na.rm = TRUE)
```

The average number of emoji's per tweet in Spanish is 2.14

#Average number of unique emoji's per tweet - Spanish

```{r}
mean(group_raw_tweets_es$unique_count, na.rm = TRUE)
```

The average number of unique emoji's per tweet in Spanish is 1.58

#Emoji entropy - English
```{r}
H=Shannon(raw_tweets_es$count)/log(2)
H
```
The entropy of emoji's in Spanish is 14.87

#top 50 tweets with emoji's
```{r}
top50_es_paris <- head(group_raw_tweets_es[with(group_raw_tweets_es,order(-count)),],50)
top50_es_paris
```

```{r}
tweets <-  top50_es_paris$text
m <- gregexpr("<[0-9a-f]{2}>", tweets)
codes <- regmatches(tweets, m)
chars <- lapply(codes, function(x) {
    rawToChar(as.raw(strtoi(paste0("0x", substr(x,2,3)))), multiple = TRUE)
})

regmatches(tweets, m) <- chars
Encoding(tweets) <- "UTF-8"
tweets
```

```{r}
write.csv(tweets,"top_50_spanish_paris.csv")
```

```{r}
rm(tdm,cleaned_corpus_es_emoji,freqSum_es)
2^H
```

##Perplexity of Spanish tweets with emoji/emoticons  is 1187

##Spanish tweets without emoji/emoticons
```{r}

### remove retweet entities
 corpus_es_NO_emoji <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_es_NO_emoji) 
 
corpus_es_NO_emoji <- Corpus(VectorSource(corpus_es_NO_emoji))
##convert text to lowercase

corpus_es_NO_emoji <- tm_map(corpus_es_NO_emoji,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_es_NO_emoji <- tm_map(corpus_es_NO_emoji,removePunctuation)


##remove numbers from text
corpus_es_NO_emoji <- tm_map(corpus_es_NO_emoji,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##cleaned_corpus_USA_en_NO_emoji <- tm_map(corpus_USA_en_NO_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_es_NO_emoji <- tm_map(corpus_es_NO_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])

cleaned_corpus_es_NO_emoji <- tm_map(cleaned_corpus_es_NO_emoji,removeWords)
cleaned_corpus_es_NO_emoji <- tm_map(cleaned_corpus_es_NO_emoji,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_es_NO_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_es_NO_emoji <- tm_map(cleaned_corpus_es_NO_emoji,stemDocument)
cleaned_corpus_es_NO_emoji <- tm_map(cleaned_corpus_es_NO_emoji,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_es_NO_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
```


#Average word length is 4.7 and average tweet length is 9.16 without emoji/emoticons


##Entropy analysis for Spanish tweets without emoji/emoticons 

```{r}
rm(words_list,wsize_per_tweet)
tdm <- TermDocumentMatrix(cleaned_corpus_es_NO_emoji)
#tdm
#tdm <- as.matrix(tdm)
freqSum_es_no_emoji <- slam::row_sums(tdm, na.rm = T)
#freqSum_France_fr 
H=Shannon(freqSum_es_no_emoji)/log(2)
H
```

##Entropy of english tweets in USA without emoji/emoticons is 10.43

Thus we can see that entropy for tweets with emoji/emoticons is slightly less than the entropy of tweets without emoji/emoticons


confidence intervals


```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_es_no_emoji, datatype = "abundance", conf = 0.95)/log(2)
```


```{r}
var
```

##Perplexity
```{r}
rm(tdm,cleaned_corpus_es_NO_emoji)
2^H
```


##Perplexity of Spanish tweets without emoji/emoticons  is 1384





