Paris

```{r}
options(stringsAsFactors = FALSE)
Sys.setlocale(category = "LC_ALL", locale = "en_US.UTF-8")
```

```{r}
#rm(data,data_non_solidarity,data_solidarity,emDict_raw)
rm(list = ls()) 
#install.packages("slam")
library(slam)
library(textcat)
#library(cldr)
library(entropart)
library(boot)
library(vegan)
library(simboot)
#update.packages()
library(tidyverse)
#library(tokenizers)
library(mgcv)
library(twitteR)
library(plyr)
library(dplyr)
library(ROAuth)
library(stringr)
library(ggplot2)
library(httr)
library(wordcloud)
library(stringi)
#library(sentiment)
library(SnowballC)
library(tm)
library(RColorBrewer)
```





```{r}
#function which will bootstrap entropies
boot.entropy <- function(data, num) {
    resamples <- lapply(1:num, function(i) sample(data, replace=T))
    H.entropy <- sapply(resamples, shannon_entropy)
      
}
```



```{r}
shannon_entropy <- function(data) {
     tdm <- TermDocumentMatrix(data)
     freqSum <- slam::row_sums(tdm, na.rm = T)
     H<-Shannon(freqSum)/log(2)
}
```

```{r}
options(stringsAsFactors = FALSE)
Sys.setlocale(category = "LC_ALL", locale = "en_US.UTF-8")
```
##English - Emoji
```{r}
corpus_en_emoji_read<-read.csv2(file="paris_en_emoji.csv",header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")

 ##Remove emoji
corpus_en_emoji <- iconv(corpus_en_emoji_read$x, "latin1", "ASCII", sub="")

### remove retweet entities
corpus_en_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_en_emoji) 


corpus_en_emoji <- Corpus(VectorSource(corpus_en_emoji))



##convert text to lowercase
corpus_en_emoji <- tm_map(corpus_en_emoji,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_en_emoji <- tm_map(corpus_en_emoji,removePunctuation)


##remove numbers from text
corpus_en_emoji <- tm_map(corpus_en_emoji,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_en_emoji <- tm_map(corpus_en_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])

##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,content_transformer(removeEmoticons))


   
##remove white space
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,stripWhitespace)
###

cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,removeWords,c('paris'))
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_en_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,stemDocument)
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_en_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
length(cleaned_corpus_en_emoji)
#inspect(cleaned_corpus_en_emoji[1:25])

```

```{r}
bootstrapped_entropies <- boot.entropy(cleaned_corpus_en_emoji,50)
```

```{r}
bootstrapped_entropies<-as.data.frame(bootstrapped_entropies)
names(bootstrapped_entropies)[1]<-"H"
```

```{r}
boot_en_emoji<-cbind(bootstrapped_entropies,lang="en",isEmoji=1)
```

```{r}
write.csv(boot_en_emoji,"boot.csv")
```


##English - without Emoji

```{r}

corpus_en_no_emoji_read<-read.csv2(file="paris_en_no_emoji.csv",header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")

### remove retweet entities
 corpus_en_NO_emoji <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_en_no_emoji_read$x) 
 
corpus_en_NO_emoji <- Corpus(VectorSource(corpus_en_NO_emoji))
##convert text to lowercase

corpus_en_NO_emoji <- tm_map(corpus_en_NO_emoji,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_en_NO_emoji <- tm_map(corpus_en_NO_emoji,removePunctuation)


##remove numbers from text
corpus_en_NO_emoji <- tm_map(corpus_en_NO_emoji,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##cleaned_corpus_USA_en_NO_emoji <- tm_map(corpus_USA_en_NO_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_en_NO_emoji <- tm_map(corpus_en_NO_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])

cleaned_corpus_en_NO_emoji <- tm_map(cleaned_corpus_en_NO_emoji,removeWords,c('paris'))
cleaned_corpus_en_NO_emoji <- tm_map(cleaned_corpus_en_NO_emoji,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_en_NO_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_en_NO_emoji <- tm_map(cleaned_corpus_en_NO_emoji,stemDocument)
cleaned_corpus_en_NO_emoji <- tm_map(cleaned_corpus_en_NO_emoji,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_en_NO_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
```


```{r}
bootstrapped_entropies <- boot.entropy(cleaned_corpus_en_NO_emoji,50)
```

```{r}
bootstrapped_entropies<-as.data.frame(bootstrapped_entropies)
names(bootstrapped_entropies)[1]<-"H"
```

```{r}
boot_en_no_emoji<-cbind(bootstrapped_entropies,lang="en",isEmoji=0)
```

```{r}
write.csv(boot_en_no_emoji,"boot_en_no_emoji.csv")
```

##French - Emoji

```{r}
corpus_fr_emoji_read<-read.csv2(file="paris_fr_emoji.csv",header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")

 ##Remove emoji
corpus_fr_emoji <- iconv(corpus_fr_emoji_read$x, "latin1", "ASCII", sub="")

### remove retweet entities
corpus_fr_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_fr_emoji) 


corpus_fr_emoji <- Corpus(VectorSource(corpus_fr_emoji))



##convert text to lowercase
corpus_fr_emoji <- tm_map(corpus_fr_emoji,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_fr_emoji <- tm_map(corpus_fr_emoji,removePunctuation)


##remove numbers from text
corpus_fr_emoji <- tm_map(corpus_fr_emoji,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_fr_emoji <- tm_map(corpus_fr_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])

##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_fr_emoji <- tm_map(cleaned_corpus_fr_emoji,content_transformer(removeEmoticons))


   
##remove white space
cleaned_corpus_fr_emoji <- tm_map(cleaned_corpus_fr_emoji,stripWhitespace)
###


cleaned_corpus_fr_emoji <- tm_map(cleaned_corpus_fr_emoji,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_fr_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_fr_emoji <- tm_map(cleaned_corpus_fr_emoji,stemDocument)
cleaned_corpus_fr_emoji <- tm_map(cleaned_corpus_fr_emoji,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_fr_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
length(cleaned_corpus_fr_emoji)
#inspect(cleaned_corpus_en_emoji[1:25])

```

```{r}
bootstrapped_entropies <- boot.entropy(cleaned_corpus_fr_emoji,50)
```

```{r}
bootstrapped_entropies<-as.data.frame(bootstrapped_entropies)
names(bootstrapped_entropies)[1]<-"H"
```

```{r}
boot_fr_emoji<-cbind(bootstrapped_entropies,lang="fr",isEmoji=1)
```

```{r}
write.csv(boot_fr_emoji,"boot_fr_emoji.csv")
```


##French- no emoji

```{r}
corpus_fr_no_emoji_read<-read.csv2(file="paris_fr_no_emoji.csv",header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")

### remove retweet entities
 corpus_fr_NO_emoji <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_fr_no_emoji_read$x) 
 
corpus_fr_NO_emoji <- Corpus(VectorSource(corpus_fr_NO_emoji))
##convert text to lowercase

corpus_fr_NO_emoji <- tm_map(corpus_fr_NO_emoji,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_fr_NO_emoji <- tm_map(corpus_fr_NO_emoji,removePunctuation)


##remove numbers from text
corpus_fr_NO_emoji <- tm_map(corpus_fr_NO_emoji,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##cleaned_corpus_USA_en_NO_emoji <- tm_map(corpus_USA_en_NO_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_fr_NO_emoji <- tm_map(corpus_fr_NO_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])


cleaned_corpus_fr_NO_emoji <- tm_map(cleaned_corpus_fr_NO_emoji,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_fr_NO_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_fr_NO_emoji <- tm_map(cleaned_corpus_fr_NO_emoji,stemDocument)
cleaned_corpus_fr_NO_emoji <- tm_map(cleaned_corpus_fr_NO_emoji,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_fr_NO_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
```

```{r}
bootstrapped_entropies <- boot.entropy(cleaned_corpus_fr_NO_emoji,50)
```

```{r}
bootstrapped_entropies<-as.data.frame(bootstrapped_entropies)
names(bootstrapped_entropies)[1]<-"H"
```

```{r}
boot_fr_no_emoji<-cbind(bootstrapped_entropies,lang="fr",isEmoji=0)
```

```{r}
write.csv(boot_fr_no_emoji,"boot_fr_no_emoji.csv")
```

##Spanish - emoji

```{r}
corpus_es_emoji_read<-read.csv2(file="paris_es_emoji.csv",header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
 ##Remove emoji
corpus_es_emoji <- iconv(corpus_es_emoji_read$x, "latin1", "ASCII", sub="")

### remove retweet entities
corpus_es_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_es_emoji) 


corpus_es_emoji <- Corpus(VectorSource(corpus_es_emoji))



##convert text to lowercase
corpus_es_emoji <- tm_map(corpus_es_emoji,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_es_emoji <- tm_map(corpus_es_emoji,removePunctuation)


##remove numbers from text
corpus_es_emoji <- tm_map(corpus_es_emoji,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_es_emoji <- tm_map(corpus_es_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])

##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_es_emoji <- tm_map(cleaned_corpus_es_emoji,content_transformer(removeEmoticons))


   
##remove white space
cleaned_corpus_es_emoji <- tm_map(cleaned_corpus_es_emoji,stripWhitespace)
###

cleaned_corpus_es_emoji <- tm_map(cleaned_corpus_es_emoji,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_es_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_es_emoji <- tm_map(cleaned_corpus_es_emoji,stemDocument)
cleaned_corpus_es_emoji <- tm_map(cleaned_corpus_es_emoji,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_es_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
length(cleaned_corpus_es_emoji)
#inspect(cleaned_corpus_en_emoji[1:25])

```

```{r}
bootstrapped_entropies <- boot.entropy(cleaned_corpus_es_emoji,50)
```

```{r}
bootstrapped_entropies<-as.data.frame(bootstrapped_entropies)
names(bootstrapped_entropies)[1]<-"H"
```

```{r}
boot_es_emoji<-cbind(bootstrapped_entropies,lang="es",isEmoji=1)
```

```{r}
write.csv(boot_es_emoji,"boot_es_emoji.csv")
```

##Spanish - no emoji

```{r}
corpus_es_no_emoji_read<-read.csv2(file="paris_es_no_emoji.csv",header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
### remove retweet entities
 corpus_es_NO_emoji <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_es_no_emoji_read$x) 
 
corpus_es_NO_emoji <- Corpus(VectorSource(corpus_es_NO_emoji))
##convert text to lowercase

corpus_es_NO_emoji <- tm_map(corpus_es_NO_emoji,tolower)
#inspect(corpus[1:5])

##remove punctuations from text
corpus_es_NO_emoji <- tm_map(corpus_es_NO_emoji,removePunctuation)


##remove numbers from text
corpus_es_NO_emoji <- tm_map(corpus_es_NO_emoji,removeNumbers)
#inspect(corpus[1:5])

##remove stopwords from french language 

##cleaned_corpus_USA_en_NO_emoji <- tm_map(corpus_USA_en_NO_emoji,removeWords,stopwords("en"))


##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_es_NO_emoji <- tm_map(corpus_es_NO_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])


cleaned_corpus_es_NO_emoji <- tm_map(cleaned_corpus_es_NO_emoji,stripWhitespace)

##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_es_NO_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len

##Stemming
cleaned_corpus_es_NO_emoji <- tm_map(cleaned_corpus_es_NO_emoji,stemDocument)
cleaned_corpus_es_NO_emoji <- tm_map(cleaned_corpus_es_NO_emoji,stripWhitespace)

##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_es_NO_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))

avg_tweet_len
```

```{r}
bootstrapped_entropies <- boot.entropy(cleaned_corpus_es_NO_emoji,50)
```

```{r}
bootstrapped_entropies<-as.data.frame(bootstrapped_entropies)
names(bootstrapped_entropies)[1]<-"H"
```

```{r}
boot_es_no_emoji<-cbind(bootstrapped_entropies,lang="es",isEmoji=0)
```

```{r}
write.csv(boot_es_no_emoji,"boot_es_no_emoji.csv")
```

##combine dataframes
```{r}
boot_en_emoji<-read.csv2(file="boot_en_emoji.csv",header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
boot_en_no_emoji<-read.csv2(file="boot_en_no_emoji.csv",header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
boot_es_emoji<-read.csv2(file="boot_es_emoji.csv",header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
boot_es_no_emoji<-read.csv2(file="boot_es_no_emoji.csv",header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
boot_fr_emoji<-read.csv2(file="boot_fr_emoji.csv",header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
boot_fr_no_emoji<-read.csv2(file="boot_fr_no_emoji.csv",header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
```
```{r}
#setwd("/Users/vsriniv6/Documents/R_scripts")
boot_fr_no_emoji<-read.csv2(file="boot_fr_no_emoji.csv",header=TRUE, sep=",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
```

```{r}
boot_paris_combined<- Reduce(function(x, y) merge(x, y, all=TRUE), list(boot_en_emoji, boot_en_no_emoji, boot_es_emoji,boot_es_no_emoji,boot_fr_emoji,boot_fr_no_emoji))
```


```{r}
boot_paris_combined
```

##Anova - Language
```{r}
summary(aov(H ~ lang, data = boot_paris_combined))
```

```{r}
summary(aov(H ~ isEmoji, data = boot_paris_combined))
```


```{r}
summary(aov(H ~ lang+isEmoji, data = boot_paris_combined))
```
```{r}
summary(aov(H ~ lang+isEmoji+lang*isEmoji, data = boot_paris_combined))
```


```{r}
interaction.plot(factorA, factorB, Response)
```
